# app.py
import os
import uuid # Job ID ìƒì„±ìš©
import threading
import asyncio
import json
import time
from flask import Flask, request, jsonify, Response, stream_with_context
import requests
from dotenv import load_dotenv # .env íŒŒì¼ ë¡œë”©ìš© (ë¡œì»¬ í…ŒìŠ¤íŠ¸)

# .env íŒŒì¼ ë¡œë“œ (Railway í™˜ê²½ ë³€ìˆ˜ê°€ ìš°ì„  ì ìš©ë¨)
load_dotenv()

# ë¡œê¹… ì„¤ì • (Railway ë¡œê·¸ í™•ì¸ì„ ìœ„í•´)
import logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# ìŠ¤í¬ë˜í•‘ í•¨ìˆ˜ import (íŒŒì¼ ì´ë¦„ ë° í•¨ìˆ˜ ì´ë¦„ í™•ì¸)
try:
    from BlogScraper import main as run_actual_scraper
except ImportError:
    logger.error("BlogScraper.py ë˜ëŠ” main í•¨ìˆ˜ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤!")
    async def run_actual_scraper(): # ì„ì‹œ í•¨ìˆ˜
        await asyncio.sleep(1)
        return [{"error": "ìŠ¤í¬ë˜í¼ ëª¨ë“ˆ ë¡œë“œ ì‹¤íŒ¨"}]

app = Flask(__name__)

# --- ìƒíƒœ ë° ê²°ê³¼ ì €ì¥ì„ ìœ„í•œ ì„ì‹œ ë©”ëª¨ë¦¬ ì €ì¥ì†Œ ---
# ì£¼ì˜: ì„œë²„ ì¬ì‹œì‘ ì‹œ ëª¨ë“  ë°ì´í„°ê°€ ì‚¬ë¼ì§‘ë‹ˆë‹¤!
# ì‹¤ì œ ì„œë¹„ìŠ¤ì—ì„œëŠ” Redis, DB ë“±ìœ¼ë¡œ êµì²´í•´ì•¼ í•©ë‹ˆë‹¤.
scrape_jobs = {} # job_idë¥¼ í‚¤ë¡œ ì‚¬ìš©

# --- ìŠ¤í¬ë˜í•‘ ë°±ê·¸ë¼ìš´ë“œ ì‘ì—… í•¨ìˆ˜ ---
def run_scrape_task(job_id):
    global scrape_jobs
    logger.info(f"[{job_id}] ë°±ê·¸ë¼ìš´ë“œ ìŠ¤í¬ë˜í•‘ ì‘ì—… ì‹œì‘.")

    # ìƒíƒœ ì—…ë°ì´íŠ¸: ì§„í–‰ ì¤‘
    scrape_jobs[job_id] = {"status": "running", "message": "ìŠ¤í¬ë˜í•‘ ì´ˆê¸°í™” ì¤‘...", "progress": 5, "result": None}

    try:
        # --- ìƒíƒœ ì—…ë°ì´íŠ¸: ë¡œê·¸ì¸ ë‹¨ê³„ (ì˜ˆì‹œ) ---
        scrape_jobs[job_id]["message"] = "ë„¤ì´ë²„ ë¡œê·¸ì¸ ì²˜ë¦¬ ì¤‘..."
        scrape_jobs[job_id]["progress"] = 10
        # ì—¬ê¸°ì„œ BlogScraper.py ë‚´ë¶€ì˜ ë¡œê·¸ì¸ ê´€ë ¨ ë¡œì§ì´ ì‹¤í–‰ëœë‹¤ê³  ê°€ì •
        # ì‹¤ì œ ì§„í–‰ë¥  ì—…ë°ì´íŠ¸ëŠ” BlogScraper.py ìˆ˜ì • í•„ìš”

        # --- asyncio ì´ë²¤íŠ¸ ë£¨í”„ ìƒì„± ë° ìŠ¤í¬ë˜í¼ ì‹¤í–‰ ---
        # ì°¸ê³ : Flask + Threading + AsyncioëŠ” ë³µì¡í•  ìˆ˜ ìˆìŒ. FastAPIê°€ ë” ì í•©.
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)

        # ì‹¤ì œ ìŠ¤í¬ë˜í•‘ í•¨ìˆ˜ í˜¸ì¶œ
        # BlogScraper.pyì˜ main()ì´ ìµœì¢… ê²°ê³¼ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•´ì•¼ í•¨
        scrape_result_data = loop.run_until_complete(run_actual_scraper())
        loop.close()

        # --- ìƒíƒœ ì—…ë°ì´íŠ¸: ì™„ë£Œ ---
        if isinstance(scrape_result_data, list):
            scrape_jobs[job_id].update({
                "status": "completed",
                "message": f"ìŠ¤í¬ë˜í•‘ ì™„ë£Œ ({len(scrape_result_data)}ê°œ í¬ìŠ¤íŠ¸ ìˆ˜ì§‘)",
                "progress": 100,
                "result": scrape_result_data # ê²°ê³¼ë¥¼ ì„ì‹œ ì €ì¥ì†Œì— ì €ì¥
            })
            logger.info(f"[{job_id}] ìŠ¤í¬ë˜í•‘ ì™„ë£Œ. ê²°ê³¼ ì €ì¥ë¨.")

            # --- ê²°ê³¼ Replitìœ¼ë¡œ ì „ì†¡ ---
            replit_callback_url = os.environ.get("REPLIT_CALLBACK_URL")
            replit_secret_key = os.environ.get("REPLIT_SECRET_KEY")

            if replit_callback_url and replit_secret_key:
                logger.info(f"[{job_id}] ìŠ¤í¬ë˜í•‘ ê²°ê³¼ Replitìœ¼ë¡œ ì „ì†¡ ì‹œë„...")
                try:
                    payload = {
                        # 'user_id': scrape_jobs[job_id].get('user_id'), # í•„ìš”í•˜ë‹¤ë©´ user_idë„ í¬í•¨
                        'job_id': job_id,
                        'result': scrape_result_data
                    }
                    headers = {
                        'Content-Type': 'application/json',
                        'X-Scraper-Secret': replit_secret_key
                    }
                    response = requests.post(replit_callback_url, json=payload, headers=headers, timeout=30)
                    if response.status_code == 200:
                        logger.info(f"[{job_id}] ê²°ê³¼ ì „ì†¡ ì„±ê³µ: {response.status_code}")
                    else:
                        logger.error(f"[{job_id}] ê²°ê³¼ ì „ì†¡ ì‹¤íŒ¨: {response.status_code} - {response.text[:100]}")
                except Exception as callback_err:
                    logger.error(f"[{job_id}] ê²°ê³¼ ì „ì†¡ ì¤‘ ì˜¤ë¥˜: {callback_err}")
            else:
                logger.warning(f"[{job_id}] Replit ì½œë°± URL ë˜ëŠ” Secret Keyê°€ ì„¤ì •ë˜ì§€ ì•Šì•„ ê²°ê³¼ ì „ì†¡ ìƒëµ.")

        else:
            raise Exception("ìŠ¤í¬ë˜í¼ í•¨ìˆ˜ê°€ ìœ íš¨í•œ ë¦¬ìŠ¤íŠ¸ ê²°ê³¼ë¥¼ ë°˜í™˜í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

    except Exception as e:
        error_message = f"ìŠ¤í¬ë˜í•‘ ì‘ì—… ì¤‘ ì˜¤ë¥˜: {str(e)}"
        logger.error(f"[{job_id}] {error_message}")
        # traceback.print_exc() # ìƒì„¸ ì˜¤ë¥˜ ë¡œê¹… í•„ìš”ì‹œ ì£¼ì„ í•´ì œ
        scrape_jobs[job_id].update({
            "status": "error",
            "message": error_message,
            "progress": -1,
            "result": None
        })

# --- API ì—”ë“œí¬ì¸íŠ¸ ---

@app.route('/')
def home():
    """ì„œë²„ ìƒíƒœ í™•ì¸ìš© ê¸°ë³¸ ì—”ë“œí¬ì¸íŠ¸"""
    return "ì•ˆë…•í•˜ì„¸ìš”! ë°ì´ì§€ ìŠ¤í¬ë˜í¼ ì„œë²„ê°€ ì‘ë™ ì¤‘ì…ë‹ˆë‹¤. ğŸ˜Š"

@app.route('/start-scrape', methods=['POST'])
def start_scrape_endpoint():
    """Replit ì•±ìœ¼ë¡œë¶€í„° ìŠ¤í¬ë˜í•‘ ì‹œì‘ ìš”ì²­ì„ ë°›ìŠµë‹ˆë‹¤."""
    global scrape_jobs
    # ìš”ì²­ ë°ì´í„°ì—ì„œ user_id ê°€ì ¸ì˜¤ê¸° (ì„ íƒì )
    # user_id = request.json.get('user_id')

    # ê³ ìœ  ì‘ì—… ID ìƒì„±
    job_id = str(uuid.uuid4())
    logger.info(f"ìŠ¤í¬ë˜í•‘ ìš”ì²­ ìˆ˜ì‹ . Job ID ìƒì„±: {job_id}")

    # ì‘ì—… ìƒíƒœ ì´ˆê¸°í™”
    scrape_jobs[job_id] = {"status": "pending", "message": "ìŠ¤í¬ë˜í•‘ ëŒ€ê¸° ì¤‘...", "progress": 0, "result": None}
    # if user_id: scrape_jobs[job_id]['user_id'] = user_id # í•„ìš”ì‹œ ì‚¬ìš©ì ID ì €ì¥

    # ë°±ê·¸ë¼ìš´ë“œ ìŠ¤ë ˆë“œì—ì„œ ìŠ¤í¬ë˜í•‘ ì‘ì—… ì‹œì‘
    thread = threading.Thread(target=run_scrape_task, args=(job_id,))
    thread.daemon = True # ë©”ì¸ ìŠ¤ë ˆë“œ ì¢…ë£Œ ì‹œ í•¨ê»˜ ì¢…ë£Œ
    thread.start()

    # Replit ì•±ì—ëŠ” ì‘ì—… IDì™€ í•¨ê»˜ ìˆ˜ë½ë˜ì—ˆìŒì„ ì•Œë¦¼
    return jsonify({"message": "ìŠ¤í¬ë˜í•‘ ì‘ì—…ì´ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤.", "job_id": job_id}), 202

@app.route('/status/<job_id>')
def status_endpoint(job_id):
    """íŠ¹ì • ì‘ì—… IDì˜ ì§„í–‰ ìƒíƒœë¥¼ SSE(Server-Sent Events)ë¡œ ìŠ¤íŠ¸ë¦¬ë°í•©ë‹ˆë‹¤."""
    global scrape_jobs
    logger.info(f"[{job_id}] ìƒíƒœ í™•ì¸ ìš”ì²­ ìˆ˜ì‹ ")

    def event_stream():
        last_status_json = None
        while True:
            job_info = scrape_jobs.get(job_id)
            if not job_info:
                # ì‘ì—… IDê°€ ìœ íš¨í•˜ì§€ ì•Šì€ ê²½ìš°
                error_data = {"status": "error", "message": "ìœ íš¨í•˜ì§€ ì•Šì€ ì‘ì—… IDì…ë‹ˆë‹¤.", "progress": -1}
                yield f"data: {json.dumps(error_data)}\n\n"
                logger.warning(f"[{job_id}] ìœ íš¨í•˜ì§€ ì•Šì€ ì‘ì—… IDë¡œ ìƒíƒœ í™•ì¸ ì‹œë„")
                break

            current_status = {k: v for k, v in job_info.items() if k != 'result'} # ê²°ê³¼ ë°ì´í„° ì œì™¸
            current_status_json = json.dumps(current_status)

            # ìƒíƒœê°€ ë³€ê²½ë˜ì—ˆì„ ë•Œë§Œ ì „ì†¡
            if current_status_json != last_status_json:
                yield f"data: {current_status_json}\n\n"
                last_status_json = current_status_json
                logger.debug(f"[{job_id}] ìƒíƒœ ì—…ë°ì´íŠ¸ ì „ì†¡: {current_status}")

            # ì‘ì—…ì´ ì™„ë£Œë˜ê±°ë‚˜ ì˜¤ë¥˜ ë°œìƒ ì‹œ ìŠ¤íŠ¸ë¦¼ ì¢…ë£Œ
            if job_info["status"] in ["completed", "error"]:
                logger.info(f"[{job_id}] ìƒíƒœ ìŠ¤íŠ¸ë¦¼ ì¢…ë£Œ (ìƒíƒœ: {job_info['status']})")
                break

            # 2ì´ˆ ê°„ê²©ìœ¼ë¡œ í™•ì¸
            time.sleep(2)

    # SSE ì‘ë‹µ ë°˜í™˜
    return Response(stream_with_context(event_stream()), mimetype="text/event-stream")

@app.route('/result/<job_id>', methods=['GET'])
def get_result_endpoint(job_id):
    """(ì„ íƒì ) ì™„ë£Œëœ ì‘ì—…ì˜ ê²°ê³¼ë¥¼ ì§ì ‘ ê°€ì ¸ì˜¤ëŠ” ì—”ë“œí¬ì¸íŠ¸"""
    global scrape_jobs
    job_info = scrape_jobs.get(job_id)

    if not job_info:
        return jsonify({"error": "ìœ íš¨í•˜ì§€ ì•Šì€ ì‘ì—… IDì…ë‹ˆë‹¤."}), 404

    if job_info["status"] == "completed":
        return jsonify({"job_id": job_id, "status": "completed", "result": job_info.get("result")})
    elif job_info["status"] == "error":
        return jsonify({"job_id": job_id, "status": "error", "message": job_info.get("message")}), 500
    else:
        return jsonify({"job_id": job_id, "status": job_info.get("status"), "message": "ì‘ì—…ì´ ì•„ì§ ì§„í–‰ ì¤‘ì…ë‹ˆë‹¤."}), 202

if __name__ == '__main__':
    # RailwayëŠ” PORT í™˜ê²½ ë³€ìˆ˜ë¥¼ ì‚¬ìš©. ë¡œì»¬ í…ŒìŠ¤íŠ¸ ì‹œ ê¸°ë³¸ 8080 ì‚¬ìš©.
    port = int(os.environ.get('PORT', 8080))
    # debug=TrueëŠ” ë¡œì»¬ í…ŒìŠ¤íŠ¸ ì‹œì—ë§Œ ìœ ìš©. ì‹¤ì œ ë°°í¬ ì‹œì—ëŠ” False ê¶Œì¥.
    app.run(host='0.0.0.0', port=port, debug=False)