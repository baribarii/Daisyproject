import asyncio
import re
import json
import traceback
import os  # <--- í™˜ê²½ ë³€ìˆ˜ ì‚¬ìš©ì„ ìœ„í•´ import
from playwright.async_api import async_playwright, TimeoutError as PlaywrightTimeoutError, Error as PlaywrightError

# â€” replit.db í´ë°±(Fallback) ì„¤ì • â€”
try:
    from replit import db
    use_replit_db = True
    print("âœ… replit.db ëª¨ë“ˆ ê°ì§€: Replit DBì— ì €ì¥í•©ë‹ˆë‹¤.")
except ModuleNotFoundError:
    use_replit_db = False
    print("âš ï¸ replit.db ëª¨ë“ˆ ì—†ìŒ: ë¡œì»¬ JSON íŒŒì¼(blog_posts.json)ì— ì €ì¥í•©ë‹ˆë‹¤.")

# --- ìŠ¤í¬ë˜í•‘ ì„¤ì •ê°’ ---
MAX_CONCURRENT_PAGES = 5
REQUEST_BLOCKING_ENABLED = True
BLOCKED_RESOURCE_TYPES = ["image", "font", "media", "stylesheet"]
BLOCKED_DOMAINS = [
    "google-analytics.com", "googlesyndication.com", "googletagmanager.com",
    "googletagservices.com", "doubleclick.net", "naver.com/ad",
    "pagead2.googlesyndication.com", "analytics.naver.com", "crto.net",
    "acecounter.com", "facebook.net", "adnxs.com", "instagram.com"
]

# --- ë„¤ì´ë²„ ê´€ë ¨ ì„¤ì • ---
NAVER_LOGIN_URL    = "https://nid.naver.com/nidlogin.login"
NAVER_LOGIN_DOMAIN = "nid.naver.com"
MY_BLOG_ALIAS_URL  = "https://blog.naver.com/MyBlog.naver"
EXPORT_URL_TPL     = "https://admin.blog.naver.com/{}/config/postexport"
IFRAME_SELECTOR    = "#papermain"
POST_ROW_SELECTOR  = "#post_list_body tr[class*='postlist']"
PAGE_LINK_SELECTOR = "a.page"
NEXT_GROUP_SELECTOR= "a.page:has-text('ë‹¤ìŒ')"

# í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•œ ìˆ˜ì§‘ ì œí•œ ì„¤ì •
MAX_POSTS_TO_COLLECT = 15
# MAX_POSTS_TO_COLLECT = None

# timeouts (ë°€ë¦¬ì´ˆ)
SHORT_TO = 15_000
MID_TO   = 40_000
LONG_TO  = 90_000

async def block_unnecessary_requests(route):
    """ë„¤íŠ¸ì›Œí¬ ìš”ì²­ì„ ê°€ë¡œì±„ ë¶ˆí•„ìš”í•œ ë¦¬ì†ŒìŠ¤ë¥¼ ì°¨ë‹¨í•˜ëŠ” í•¨ìˆ˜"""
    request = route.request
    resource_type = request.resource_type
    url = request.url

    if resource_type in BLOCKED_RESOURCE_TYPES:
        try: await route.abort()
        except PlaywrightError: pass
        return

    if any(domain in url for domain in BLOCKED_DOMAINS):
        try: await route.abort()
        except PlaywrightError: pass
        return

    try: await route.continue_()
    except PlaywrightError: pass

async def scrape_single_post(context, meta, idx, total_posts, semaphore):
    """ë‹¨ì¼ ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ì˜ ë³¸ë¬¸ì„ ìŠ¤í¬ë˜í•‘í•˜ëŠ” ë¹„ë™ê¸° í•¨ìˆ˜ (iframe ìš°ì„  íƒìƒ‰)"""
    async with semaphore:
        print(f"  [{idx}/{total_posts}] ì‹œì‘: {meta['url']}")
        content = ""
        p2 = None
        data = {**meta, "content": "ì¶”ì¶œ ì‹œì‘ ì „"}

        try:
            p2 = await context.new_page()
            if REQUEST_BLOCKING_ENABLED:
                await p2.route("**/*", block_unnecessary_requests)

            await p2.goto(meta["url"], timeout=LONG_TO, wait_until="domcontentloaded")

            content_found = False
            extracted_content = ""

            # --- iframe ìš°ì„  íƒìƒ‰ ---
            try:
                main_iframe_locator = p2.locator('#mainFrame')
                await main_iframe_locator.wait_for(state="visible", timeout=MID_TO)
                print(f"  âœ“ [{idx}/{total_posts}] #mainFrame iframe ë°œê²¬. ë‚´ë¶€ í™•ì¸ ì¤‘...")

                frame = main_iframe_locator.frame_locator(':scope')
                iframe_content_locator = frame.locator('#postViewArea, .se-main-container')
                await iframe_content_locator.first.wait_for(state="visible", timeout=MID_TO)

                iframe_post_view = frame.locator('#postViewArea')
                if await iframe_post_view.count() > 0:
                    extracted_content = await iframe_post_view.first.inner_text(timeout=SHORT_TO)
                    if extracted_content.strip():
                        print(f"  âœ“ [{idx}/{total_posts}] iframe ì¶”ì¶œ ì„±ê³µ (#postViewArea)")
                        content_found = True

                if not content_found:
                     iframe_main_container = frame.locator('.se-main-container')
                     if await iframe_main_container.count() > 0:
                        extracted_content = await iframe_main_container.first.inner_text(timeout=SHORT_TO)
                        if extracted_content.strip():
                            print(f"  âœ“ [{idx}/{total_posts}] iframe ì¶”ì¶œ ì„±ê³µ (.se-main-container)")
                            content_found = True

                if not content_found:
                     print(f"  âš ï¸ [{idx}/{total_posts}] iframe ë‚´ì—ì„œ ì½˜í…ì¸  ìš”ì†Œ(#postViewArea, .se-main-container) ì°¾ê¸° ì‹¤íŒ¨")


            except PlaywrightTimeoutError:
                 print(f"  âš ï¸ [{idx}/{total_posts}] iframe ë˜ëŠ” ë‚´ë¶€ ì½˜í…ì¸  ë¡œë”© íƒ€ì„ì•„ì›ƒ.")
            except Exception as e:
                 # iframeì´ ì•„ì˜ˆ ì—†ëŠ” ê²½ìš° ë“± ì˜¤ë¥˜ ë°œìƒ ê°€ëŠ¥
                 print(f"  âš ï¸ [{idx}/{total_posts}] iframe ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")


            # --- JavaScript ìµœí›„ ì‹œë„ (iframe ìš°ì„  í™•ì¸) ---
            if not content_found:
                print(f"  [{idx}/{total_posts}] JavaScript ì¶”ì¶œ ì‹œë„...")
                try:
                    js_content = await p2.evaluate("""
                        () => {
                            let content = '';
                            const iframe = document.querySelector('#mainFrame');
                            if (iframe && iframe.contentDocument) {
                                const iframePostView = iframe.contentDocument.querySelector('#postViewArea');
                                if (iframePostView && iframePostView.innerText.trim()) { content = iframePostView.innerText; }
                                if (!content.trim()) {
                                    const iframeMainContainer = iframe.contentDocument.querySelector('.se-main-container');
                                    if (iframeMainContainer && iframeMainContainer.innerText.trim()) { content = iframeMainContainer.innerText; }
                                }
                            }
                            if (!content.trim()) {
                                const postView = document.querySelector('#postViewArea');
                                if (postView && postView.innerText.trim()) { content = postView.innerText; }
                            }
                            if (!content.trim()) {
                                const mainContainer = document.querySelector('.se-main-container');
                                if (mainContainer && mainContainer.innerText.trim()) { content = mainContainer.innerText; }
                            }
                            return content;
                        }
                    """, timeout=SHORT_TO)

                    if js_content and js_content.strip():
                        extracted_content = js_content
                        content_found = True
                        print(f"  âœ“ [{idx}/{total_posts}] JavaScript ì¶”ì¶œ ì„±ê³µ")
                    else:
                        print(f"  âš ï¸ [{idx}/{total_posts}] JavaScriptë¡œë„ ì½˜í…ì¸  ì°¾ì§€ ëª»í•¨.")
                except Exception as e:
                    print(f"  âš ï¸ [{idx}/{total_posts}] JavaScript ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")

            # --- ìµœì¢… ê²°ê³¼ ì²˜ë¦¬ ---
            if content_found:
                content = extracted_content.strip()
                data["content"] = content
            else:
                print(f"  âŒ [{idx}/{total_posts}] ëª¨ë“  ë°©ë²• ì‹¤íŒ¨: {meta['url']}")
                data["content"] = "ë³¸ë¬¸ ë‚´ìš© ì¶”ì¶œ ì‹¤íŒ¨"
                try:
                    screenshot_path = f'debug_post_{meta["logNo"]}.png' # ë¡œì»¬ ì €ì¥ ê²½ë¡œ
                    # Railway ê°™ì€ í™˜ê²½ì—ì„œëŠ” íŒŒì¼ ì‹œìŠ¤í…œ ì“°ê¸°ê°€ ì œí•œë  ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì˜¤ë¥˜ ì²˜ë¦¬
                    await p2.screenshot(path=screenshot_path, full_page=True)
                    print(f"  â†’ [{idx}/{total_posts}] ë””ë²„ê¹… ìŠ¤í¬ë¦°ìƒ· ì €ì¥: {screenshot_path}")
                except Exception as ss_err:
                    print(f"  âš ï¸ [{idx}/{total_posts}] ìŠ¤í¬ë¦°ìƒ· ì €ì¥ ì‹¤íŒ¨: {ss_err}")

            print(f"  [{idx}/{total_posts}] ì™„ë£Œ: {meta['url']}")
            await p2.close()
            return data

        except PlaywrightError as pe:
            print(f"âŒ [{idx}/{total_posts}] Playwright ì˜¤ë¥˜: {pe} | URL: {meta['url']}")
            data["content"] = f"Playwright ì˜¤ë¥˜ë¡œ ì¸í•œ ì¶”ì¶œ ì‹¤íŒ¨: {str(pe)[:100]}"
            if p2 and not p2.is_closed():
                try: await p2.close()
                except Exception: pass
            return data
        except Exception as e:
            print(f"âŒ [{idx}/{total_posts}] ì˜ˆê¸°ì¹˜ ì•Šì€ ì˜¤ë¥˜: {e} | URL: {meta['url']}")
            data["content"] = f"ì˜¤ë¥˜ë¡œ ì¸í•œ ì¶”ì¶œ ì‹¤íŒ¨: {str(e)[:100]}"
            if p2 and not p2.is_closed():
                try: await p2.close()
                except Exception: pass
            return data
        finally:
            await asyncio.sleep(0.2)


async def main():
    all_meta = []
    pw = None
    browser = None
    # final_posts_dataë¥¼ try ë¸”ë¡ ì „ì— ì´ˆê¸°í™”
    final_posts_data = []

    try:
        # --- Proxy Configuration Logic ---
        proxy_server_env = os.environ.get("PROXY_SERVER")
        proxy_username_env = os.environ.get("PROXY_USERNAME")
        proxy_password_env = os.environ.get("PROXY_PASSWORD")

        proxy_config = None # ê¸°ë³¸ê°’: í”„ë¡ì‹œ ì—†ìŒ

        if proxy_server_env: # í™˜ê²½ ë³€ìˆ˜ê°€ ìˆìœ¼ë©´ í´ë¼ìš°ë“œë¡œ ê°„ì£¼
            print("â˜ï¸ í´ë¼ìš°ë“œ í™˜ê²½ ê°ì§€ë¨. í™˜ê²½ ë³€ìˆ˜ì—ì„œ í”„ë¡ì‹œ ì„¤ì • ë¡œë“œ ì¤‘...")
            proxy_config = {
                "server": proxy_server_env,
                "username": proxy_username_env,
                "password": proxy_password_env
            }
            if not proxy_username_env or not proxy_password_env:
                 print("âš ï¸ ê²½ê³ : PROXY_USERNAME ë˜ëŠ” PROXY_PASSWORD í™˜ê²½ ë³€ìˆ˜ê°€ ì—†ìŠµë‹ˆë‹¤.")
        else: # í™˜ê²½ ë³€ìˆ˜ê°€ ì—†ìœ¼ë©´ ë¡œì»¬ë¡œ ê°„ì£¼í•˜ê³  í•˜ë“œì½”ë”©ëœ ê°’ ì‚¬ìš©
            print("ğŸ  ë¡œì»¬ í™˜ê²½ ê°ì§€ë¨. í•˜ë“œì½”ë”©ëœ í”„ë¡ì‹œ ì„¤ì • ì‚¬ìš© ì¤‘...")
            proxy_config = {
                "server": "http://168.199.145.165:6423",
                "username": "zqdduggo",
                "password": "r0i4xzlefyox"
            }

        if proxy_config:
             print(f"   - í”„ë¡ì‹œ ì„œë²„: {proxy_config['server']}")
        else:
             print("   - í”„ë¡ì‹œ ì‚¬ìš© ì•ˆ í•¨.")
        # --- End of Proxy Configuration Logic ---

        pw      = await async_playwright().start()
        # --- í´ë¼ìš°ë“œ ë°°í¬ ì‹œ headless=Trueë¡œ ë³€ê²½ ê¶Œì¥ ---
        browser = await pw.chromium.launch(
            headless=True, # Railway ë°°í¬ ì‹œ Trueë¡œ ë³€ê²½!
            slow_mo=50,     # ë°°í¬ ì‹œ 0 ë˜ëŠ” ì œê±° ê¶Œì¥
            proxy=proxy_config
        )
        context = await browser.new_context(
             viewport={"width":1280,"height":800},
             user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36"
        )
        page    = await context.new_page()

        # --- ë„¤ì´ë²„ ë¡œê·¸ì¸ (ìˆ˜ë™ ì²˜ë¦¬ ë¶€ë¶„ - ì‹¤ì œ ì„œë²„ì—ì„œëŠ” ë‹¤ë¥¸ ë°©ì‹ í•„ìš”) ---
        # ì‹¤ì œ ì„œë²„ì—ì„œëŠ” ì‚¬ìš©ìê°€ ì§ì ‘ ìƒí˜¸ì‘ìš©í•  ìˆ˜ ì—†ìœ¼ë¯€ë¡œ,
        # ì´ ë¶€ë¶„ì€ API ìš”ì²­ìœ¼ë¡œ ID/PWë¥¼ ë°›ê±°ë‚˜, ë¯¸ë¦¬ ì €ì¥ëœ ì„¸ì…˜/ì¿ í‚¤ë¥¼ ì‚¬ìš©í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ë³€ê²½í•´ì•¼ í•¨.
        # ì—¬ê¸°ì„œëŠ” ë¡œì»¬ ì‹¤í–‰ ì‹œ ìˆ˜ë™ ë¡œê·¸ì¸ì„ ê°€ì •.
        print("ğŸ”‘ ë„¤ì´ë²„ ë¡œê·¸ì¸ í˜ì´ì§€ ì—´ê¸°...")
        await page.goto(NAVER_LOGIN_URL, timeout=LONG_TO)
        print("ğŸ‘‰ í—¤ë“œë¦¬ìŠ¤ ëª¨ë“œì—ì„œëŠ” ìë™ ë¡œê·¸ì¸ì´ êµ¬í˜„ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.")
        print("   (í˜„ì¬ ì½”ë“œëŠ” ìˆ˜ë™ ë¡œê·¸ì¸ì„ ê°€ì •í•˜ë¯€ë¡œ í´ë¼ìš°ë“œ ì‹¤í–‰ ì‹œ ì´ ë¶€ë¶„ì—ì„œ ë©ˆì¶œ ìˆ˜ ìˆìŠµë‹ˆë‹¤.)")
        print("   (ì„œë²„ í™˜ê²½ì—ì„œëŠ” ID/PWë¥¼ ì§ì ‘ ì…ë ¥í•˜ê±°ë‚˜ ì¿ í‚¤/í† í°ì„ ì‚¬ìš©í•˜ëŠ” ë¡œì§ í•„ìš”)")
        # ë¡œê·¸ì¸ ì™„ë£Œ ëŒ€ê¸° (URL ë³€ê²½ ê°ì§€)
        await page.wait_for_url(lambda url: NAVER_LOGIN_DOMAIN not in url and "naver.com" in url, timeout=LONG_TO)
        print("âœ… ë¡œê·¸ì¸ ì„±ê³µ (ë˜ëŠ” ë¡œê·¸ì¸ëœ ì„¸ì…˜ ê°ì§€ë¨).")

        # 2) blogId ì¶”ì¶œ
        print("ğŸ“ ë‚´ ë¸”ë¡œê·¸ë¡œ ì´ë™í•˜ì—¬ blogId ì¶”ì¶œ...")
        await page.goto(MY_BLOG_ALIAS_URL, timeout=LONG_TO)
        await page.wait_for_load_state("networkidle", timeout=MID_TO)
        m = re.search(r"(?:blog|admin\.blog)\.naver\.com/([^/?&#]+)", page.url)
        if not m:
            iframe_url = await page.evaluate("() => document.querySelector('#mainFrame')?.src")
            if iframe_url: m = re.search(r"blog\.naver\.com/([^/?&#]+)", iframe_url)
            if not m: raise RuntimeError("âŒ blogId ì¶”ì¶œ ì‹¤íŒ¨: URL ë° iframeì—ì„œ íŒ¨í„´ ë¶ˆì¼ì¹˜")
        blog_id = m.group(1)
        print(f"âœ… blogId: {blog_id}")

        # 3) ê¸€ ì €ì¥ í˜ì´ì§€ë¡œ ì´ë™ (ë©”íƒ€ ì •ë³´ ìˆ˜ì§‘ìš©)
        export_url = EXPORT_URL_TPL.format(blog_id)
        print(f"ğŸ”— ê¸€ ì €ì¥ í˜ì´ì§€ë¡œ ì´ë™: {export_url}")
        await page.goto(export_url, timeout=LONG_TO)
        await page.wait_for_load_state("networkidle", timeout=MID_TO)

        # 4) iframe ë‚´ë¶€ í”„ë ˆì„ ì–»ê¸°
        print("iframe ìš”ì†Œ ëŒ€ê¸° ì¤‘â€¦")
        iframe_el = await page.wait_for_selector(IFRAME_SELECTOR, state="attached", timeout=LONG_TO)
        frame     = await iframe_el.content_frame()
        if not frame: raise RuntimeError("âŒ iframe.content_frame() ì‹¤íŒ¨")
        print("âœ… iframe ë‚´ë¶€ í”„ë ˆì„ íšë“ ì™„ë£Œ.")

        # 5) ë©”íƒ€ ì •ë³´ ìˆ˜ì§‘ í•¨ìˆ˜ (ë‚´ë¶€ ì •ì˜)
        async def scrape_meta_page():
            nonlocal all_meta, blog_id
            await asyncio.sleep(0.5)
            try:
                page_meta_data = await frame.evaluate("""
                    (args) => {
                        const selector = args[0];
                        const blogId = args[1];
                        const rows = document.querySelectorAll(selector);
                        const data = [];
                        rows.forEach(row => {
                            const logno = row.getAttribute('logno');
                            const dateEl = row.querySelector('td.tc span.num.add_date');
                            const titleLink = row.querySelector('span.txt.title a');
                            let url = null;
                            if (titleLink) { url = titleLink.href; }
                            else if (logno && blogId) { url = `https://blog.naver.com/${blogId}/${logno}`; }
                            data.push({
                                logno: logno,
                                date: dateEl ? dateEl.textContent.trim() : null,
                                title: titleLink ? titleLink.innerText.trim() : 'ì œëª© ì—†ìŒ',
                                url: url
                            });
                        });
                        return data;
                    }
                """, [POST_ROW_SELECTOR, blog_id])

                print(f"  - í˜„ì¬ í˜ì´ì§€ì—ì„œ {len(page_meta_data)}ê°œì˜ í–‰ ë°ì´í„° ë°œê²¬ (evaluate)")
                found_new = 0
                for item in page_meta_data:
                    logno = item.get('logno')
                    date_str = item.get('date')
                    title = item.get('title', 'ì œëª© ì—†ìŒ')
                    url = item.get('url')
                    if not logno: continue
                    if not date_str: date_str = "ë‚ ì§œ ì—†ìŒ"
                    if not url: url = f"https://blog.naver.com/{blog_id}/{logno}"
                    if not any(p["logNo"] == logno for p in all_meta):
                        all_meta.append({"logNo": logno, "title": title, "url": url, "date": date_str})
                        print(f"    âœ“ ìˆ˜ì§‘: {logno} - {title[:30]}...")
                        found_new += 1
                        if MAX_POSTS_TO_COLLECT and len(all_meta) >= MAX_POSTS_TO_COLLECT:
                            print(f"ğŸ›‘ ìˆ˜ì§‘ ì œí•œ ë„ë‹¬: {MAX_POSTS_TO_COLLECT}ê°œ")
                            return True
                print(f"  - ìƒˆë¡œìš´ ë©”íƒ€ {found_new}ê°œ ì¶”ê°€ë¨.")
                return False
            except PlaywrightTimeoutError as te:
                print(f"  âŒ evaluate ì‹¤í–‰ ì¤‘ íƒ€ì„ì•„ì›ƒ ë°œìƒ: {te}")
                return False
            except Exception as e:
                print(f"  âŒ ë©”íƒ€ ì •ë³´ ìŠ¤í¬ë˜í•‘ ì¤‘ ì˜¤ë¥˜ (evaluate): {e}")
                traceback.print_exc()
                return False

        # â€” ë©”íƒ€ ì •ë³´ ìŠ¤í¬ë˜í•‘ ì‹œì‘ ë° í˜ì´ì§€ë„¤ì´ì…˜ â€”
        print("ğŸš€ ë©”íƒ€ ì •ë³´ ìŠ¤í¬ë˜í•‘ ì‹œì‘...")
        try:
            await frame.locator(POST_ROW_SELECTOR).first.wait_for(state="attached", timeout=MID_TO)
            print("  - ì²« í˜ì´ì§€ ë¡œë”© í™•ì¸ë¨.")
        except PlaywrightTimeoutError:
            print("âš ï¸ ì²« í˜ì´ì§€ ë¡œë”© ì‹¤íŒ¨ ë˜ëŠ” ê²Œì‹œê¸€ ì—†ìŒ.")

        should_stop = await scrape_meta_page()

        if not should_stop:
            group = 1
            current_page = 1
            while True:
                if MAX_POSTS_TO_COLLECT and len(all_meta) >= MAX_POSTS_TO_COLLECT:
                    print(f"ğŸ›‘ ìˆ˜ì§‘ ì œí•œ ë„ë‹¬ë¡œ í˜ì´ì§€ë„¤ì´ì…˜ ì¤‘ë‹¨: {MAX_POSTS_TO_COLLECT}ê°œ")
                    break
                page_numbers_texts = await frame.locator(PAGE_LINK_SELECTOR).all_inner_texts()
                number_pages = []
                for text in page_numbers_texts:
                    if text.isdigit():
                        page_num = int(text)
                        if page_num > current_page: number_pages.append(page_num)
                number_pages.sort()
                for page_num in number_pages:
                    if MAX_POSTS_TO_COLLECT and len(all_meta) >= MAX_POSTS_TO_COLLECT: break
                    print(f"â¡ï¸ ê·¸ë£¹{group} í˜ì´ì§€ {page_num} ìŠ¤í¬ë˜í•‘...")
                    try:
                        await frame.locator(f"{PAGE_LINK_SELECTOR} >> text='{page_num}'").first.click()
                        await frame.locator(POST_ROW_SELECTOR).first.wait_for(state="attached", timeout=MID_TO)
                        current_page = page_num
                        should_stop = await scrape_meta_page()
                        if should_stop: break
                    except Exception as e:
                        print(f"âš ï¸ í˜ì´ì§€ {page_num} ì´ë™/ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                if should_stop or (MAX_POSTS_TO_COLLECT and len(all_meta) >= MAX_POSTS_TO_COLLECT): break
                next_btn = frame.locator(NEXT_GROUP_SELECTOR)
                if await next_btn.count() == 0:
                    print("ğŸ‰ ëª¨ë“  í˜ì´ì§€ ê·¸ë£¹ ìŠ¤í¬ë˜í•‘ ì™„ë£Œ.")
                    break
                print(f"â¡ï¸ ê·¸ë£¹ {group} ë â†’ 'ë‹¤ìŒ' í´ë¦­í•˜ì—¬ ë‹¤ìŒ ê·¸ë£¹ ì§„ì…...")
                try:
                    await next_btn.first.click()
                    await frame.locator(POST_ROW_SELECTOR).first.wait_for(state="attached", timeout=MID_TO)
                    group += 1
                    next_group_pages = await frame.locator(f"{PAGE_LINK_SELECTOR}").all_inner_texts()
                    numeric_pages = [int(p) for p in next_group_pages if p.isdigit()]
                    current_page = min(numeric_pages) if numeric_pages else current_page + 1
                    print(f"  - ê·¸ë£¹ {group} ì§„ì…, í˜„ì¬ í˜ì´ì§€: {current_page}")
                    should_stop = await scrape_meta_page()
                    if should_stop: break
                except Exception as e:
                    print(f"âš ï¸ 'ë‹¤ìŒ' ê·¸ë£¹ ì´ë™ ë˜ëŠ” ë¡œë”© ì‹¤íŒ¨: {e}")
                    break
        print(f"\nğŸ“‹ ë©”íƒ€ ì´ {len(all_meta)}ê°œ ìˆ˜ì§‘ ì™„ë£Œ.")

        # 6) ë³¸ë¬¸ ë‚´ìš© ë™ì‹œ ìŠ¤í¬ë˜í•‘ ë° ì €ì¥
        if not all_meta:
             print("â„¹ï¸ ìˆ˜ì§‘ëœ ë©”íƒ€ ì •ë³´ê°€ ì—†ì–´ ë³¸ë¬¸ ìŠ¤í¬ë˜í•‘ì„ ê±´ë„ˆ<0xEB><0x9C><0x85>ë‹ˆë‹¤.")
        else:
            print(f"\nğŸš€ {len(all_meta)}ê°œ í¬ìŠ¤íŠ¸ ë³¸ë¬¸ ë™ì‹œ ìŠ¤í¬ë˜í•‘ ì‹œì‘ (ìµœëŒ€ ë™ì‹œ: {MAX_CONCURRENT_PAGES}ê°œ)...")
            semaphore = asyncio.Semaphore(MAX_CONCURRENT_PAGES)
            tasks = []
            total_posts = len(all_meta)
            for idx, meta in enumerate(all_meta, start=1):
                task = asyncio.create_task(scrape_single_post(context, meta, idx, total_posts, semaphore))
                tasks.append(task)
            results = await asyncio.gather(*tasks, return_exceptions=True)
            print("\nâœ… ëª¨ë“  ë³¸ë¬¸ ìŠ¤í¬ë˜í•‘ ì‘ì—… ì™„ë£Œ. ê²°ê³¼ ì²˜ë¦¬ ì¤‘...")

            # --- final_posts_data ë¦¬ìŠ¤íŠ¸ì— ê²°ê³¼ ì €ì¥ ---
            successful_count = 0
            failed_count = 0
            for i, result in enumerate(results):
                meta_info = all_meta[i] if i < len(all_meta) else {"logNo": "N/A", "title": "Unknown"}
                if isinstance(result, Exception):
                    print(f"  - ì‹¬ê°í•œ ì˜¤ë¥˜ ë°œìƒ(gather): {result} - ì—°ê´€ ë©”íƒ€: {meta_info}")
                    failed_count += 1
                    error_data = {**meta_info, "content": f"ìŠ¤í¬ë˜í•‘ ì‘ì—… ì˜¤ë¥˜: {str(result)[:100]}"}
                    final_posts_data.append(error_data) # ì‹¤íŒ¨ ë°ì´í„°ë„ í¬í•¨
                elif isinstance(result, dict):
                    final_posts_data.append(result) # ì„±ê³µ/ì‹¤íŒ¨ ê²°ê³¼ dict í¬í•¨
                    if "ì¶”ì¶œ ì‹¤íŒ¨" in result.get("content", "") or "ì˜¤ë¥˜ë¡œ ì¸í•œ" in result.get("content", ""):
                        failed_count += 1
                    else: successful_count += 1
                else:
                    print(f"  - ì•Œ ìˆ˜ ì—†ëŠ” ê²°ê³¼ íƒ€ì…: {type(result)} - ì—°ê´€ ë©”íƒ€: {meta_info}")
                    failed_count += 1
                    unknown_data = {**meta_info, "content": "ì•Œ ìˆ˜ ì—†ëŠ” ê²°ê³¼ íƒ€ì…"}
                    final_posts_data.append(unknown_data)
            print(f"ğŸ“Š ìŠ¤í¬ë˜í•‘ ê²°ê³¼: ì„±ê³µ {successful_count}ê°œ, ì‹¤íŒ¨ {failed_count}ê°œ")

            # 7) ìµœì¢… ë°ì´í„° ì €ì¥ (ì„ íƒì  - app.pyê°€ ê²°ê³¼ë¥¼ ë°›ì•„ ì²˜ë¦¬í•  ê²ƒì´ë¯€ë¡œ ì£¼ì„ ì²˜ë¦¬ ê°€ëŠ¥)
            if not use_replit_db: # ë¡œì»¬ íŒŒì¼ ì €ì¥ì€ ë¡œì»¬ í…ŒìŠ¤íŠ¸ ì‹œì—ë§Œ ì˜ë¯¸ ìˆìŒ
                output_filename = "blog_posts.json"
                try:
                    with open(output_filename, "w", encoding="utf-8") as f:
                        json.dump(final_posts_data, f, ensure_ascii=False, indent=2)
                    print(f"âœ… (ë¡œì»¬ í…ŒìŠ¤íŠ¸ìš©) {output_filename}ì— {len(final_posts_data)}ê°œ í¬ìŠ¤íŠ¸ ì €ì¥ ì™„ë£Œ.")
                except IOError as io_err:
                    print(f"âŒ ë¡œì»¬ íŒŒì¼ ì €ì¥ ì‹¤íŒ¨ ({output_filename}): {io_err}")

        # --- !!! ì„±ê³µ ì‹œ ë°˜í™˜ ë¡œì§ì„ try ë¸”ë¡ ëìœ¼ë¡œ ì´ë™ !!! ---
        print(f"BlogScraper.py: ì´ {len(final_posts_data)}ê°œ í¬ìŠ¤íŠ¸ ë°ì´í„° ë°˜í™˜")
        return final_posts_data # <--- ìŠ¤í¬ë˜í•‘ ê²°ê³¼ë¥¼ ë°˜í™˜í•´ì•¼ í•¨!

    except RuntimeError as err:
        print(f"ğŸ’¥ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {err}"); traceback.print_exc()
        return [] # ì˜¤ë¥˜ ì‹œ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
    except PlaywrightError as pe:
        print(f"ğŸ’¥ Playwright ê´€ë ¨ ì˜¤ë¥˜ ë°œìƒ: {pe}"); traceback.print_exc()
        return [] # ì˜¤ë¥˜ ì‹œ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
    except Exception as e:
        print(f"ğŸ’¥ ì˜ˆìƒì¹˜ ëª»í•œ ì¹˜ëª…ì  ì˜¤ë¥˜ ë°œìƒ: {e}"); traceback.print_exc()
        return [] # ì˜¤ë¥˜ ì‹œ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
    finally:
        # --- finally ë¸”ë¡: ì„±ê³µí•˜ë“  ì‹¤íŒ¨í•˜ë“  í•­ìƒ ì‹¤í–‰ë¨ ---
        print("ğŸ”„ ìŠ¤í¬ë˜í•‘ ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì¤‘...")
        if browser and not browser.is_closed():
            try:
                await browser.close()
                print("  - ë¸Œë¼ìš°ì € ë‹«í˜.")
            except Exception as close_err:
                print(f"  âš ï¸ ë¸Œë¼ìš°ì € ë‹«ê¸° ì˜¤ë¥˜: {close_err}")
        if pw:
            try:
                await pw.stop()
                print("  - Playwright í”„ë¡œì„¸ìŠ¤ ì¤‘ì§€ë¨.")
            except Exception as stop_err:
                 print(f"  âš ï¸ Playwright ì¤‘ì§€ ì˜¤ë¥˜: {stop_err}")
        print("ğŸ ìŠ¤í¬ë˜í•‘ ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ.")

if __name__ == "__main__":
    # ì´ íŒŒì¼ì´ ì§ì ‘ ì‹¤í–‰ë  ë•Œ (í…ŒìŠ¤íŠ¸ìš©)
    print("ìŠ¤í¬ë¦½íŠ¸ ì§ì ‘ ì‹¤í–‰ ì‹œì‘ (í…ŒìŠ¤íŠ¸ ëª¨ë“œ)")
    results = asyncio.run(main())
    print(f"\nìŠ¤í¬ë¦½íŠ¸ ì§ì ‘ ì‹¤í–‰ ì™„ë£Œ. ê²°ê³¼({len(results)}ê°œ í¬ìŠ¤íŠ¸) í™•ì¸.")
    # í…ŒìŠ¤íŠ¸ ê²°ê³¼ ê°„ë‹¨íˆ ì¶œë ¥
    if results:
        print("\n--- ìˆ˜ì§‘ëœ í¬ìŠ¤íŠ¸ (ì¼ë¶€) ---")
        for i, post in enumerate(results[:3]): # ì²˜ìŒ 3ê°œë§Œ ì¶œë ¥
            print(f"  {i+1}. ì œëª©: {post.get('title', 'N/A')[:30]}...")
            print(f"     ë‚ ì§œ: {post.get('date', 'N/A')}")
            print(f"     ë‚´ìš©: {post.get('content', '')[:50]}...")